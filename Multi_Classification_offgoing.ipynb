{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import required packages \n",
    "from __future__ import division, print_function # Imports from __future__ since we're running Python 2\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "random_state=0\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "rng = np.random.RandomState(seed=random_state)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_data = os.path.join(os.getcwd(),'train_data.csv')\n",
    "flights_train = pd.read_csv(path_data, delimiter = ',')\n",
    "path_data = os.path.join(os.getcwd(),'test_data.csv')\n",
    "flights_test = pd.read_csv(path_data, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_full = flights_train.drop('ARR_DELAY_GROUP', axis=1).values.astype(np.float) # Training features\n",
    "y_train_full = flights_train['ARR_DELAY_GROUP'].values # Training labels\n",
    "X_test = flights_test.drop('ARR_DELAY_GROUP', axis=1).values.astype(np.float) # Training features\n",
    "y_test = flights_test['ARR_DELAY_GROUP'].values # Training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, \n",
    "                                                  test_size=0.2, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = StandardScaler().fit(X_train)\n",
    "X_train_sc = sc.transform(X_train)\n",
    "X_val_sc = sc.transform(X_val)\n",
    "X_test_sc = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification performance on validation set:\n",
      "Random Forest, accuracy: 0.551, log-loss: 1.171\n",
      "Naive Bayes, accuracy: 0.416, log-loss: 6.069\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import mode # Computes the mode of a signal\n",
    "from sklearn.metrics import accuracy_score\n",
    "names = [ \"Random Forest\",\"Naive Bayes\"]\n",
    "classifiers = [\n",
    "    LogisticRegression(),\n",
    "    KNeighborsClassifier(n_neighbors=9),\n",
    "    SVC(kernel=\"linear\", probability=True, random_state=random_state),\n",
    "    SVC(kernel='rbf', probability=True, random_state=random_state),\n",
    "    DecisionTreeClassifier(max_depth=10),\n",
    "    RandomForestClassifier(max_depth=10, n_estimators=50,random_state=random_state),\n",
    "    MLPClassifier(random_state=random_state),\n",
    "    GaussianNB(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "ca_score = {} # Classification accuracy\n",
    "ce_score = {} # Cross-entropy\n",
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(X_train_sc, y_train)\n",
    "    ca_score[name] = clf.score(X_val_sc, y_val)\n",
    "    ce_score[name] = log_loss(y_val, clf.predict_proba(X_val_sc))\n",
    "print('Classification performance on validation set:')\n",
    "for clf in names:\n",
    "    print (\"{}, accuracy: {:.3f}, log-loss: {:.3f}\".format(clf, ca_score[clf], ce_score[clf]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline classification accuracy on validation set (most frequent class): 0.328\n",
      "Baseline classification accuracy on validation set (random prediction): 0.082\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "# Your code goes here\n",
    "\n",
    "# most_frequent strategy\n",
    "dcl_mf = DummyClassifier(strategy='most_frequent')\n",
    "dcl_mf.fit(X_train_sc, y_train) # Clf user guide for alternative strategy options\n",
    "y_most_frequent = dcl_mf.predict(X_val)\n",
    "\n",
    "# uniformly random prediction strategy\n",
    "# Set random_state parameter to ensure reproducibility\n",
    "dcl_rnd = DummyClassifier(strategy='uniform', random_state=10).fit(X_train_sc, y_train) \n",
    "y_random = dcl_rnd.predict(X_val)\n",
    "\n",
    "print(\"Baseline classification accuracy on validation set (most frequent class): {:.3f}\".\n",
    "      format(accuracy_score(y_val, y_most_frequent)))\n",
    "print(\"Baseline classification accuracy on validation set (random prediction): {:.3f}\".\n",
    "      format(accuracy_score(y_val, y_random)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-loss scores:\n",
      "Dummy classifier 23.204374141264378\n"
     ]
    }
   ],
   "source": [
    "pred_proba_dummy = dcl_mf.predict_proba(X_val_sc)\n",
    "print(\"Log-loss scores:\\nDummy classifier {}\".\n",
    "      format(log_loss(y_val, pred_proba_dummy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score, make_scorer\n",
    "kappa_scorer = make_scorer(cohen_kappa_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification performance on validation set:\n",
      "Decision Trees, accuracy: 0.545, log-loss: 1.731, kappa score: 0.421\n",
      "Random Forest, accuracy: 0.533, log-loss: 1.395, kappa score: 0.388\n"
     ]
    }
   ],
   "source": [
    "names = [\"Decision Trees\", \"Random Forest\"]\n",
    "classifiers = [\n",
    "    DecisionTreeClassifier(max_depth=10),\n",
    "    RandomForestClassifier(max_depth=10, n_estimators=50,random_state=random_state)]\n",
    "ca_score = {} # Classification accuracy\n",
    "ce_score = {} # Cross-entropy\n",
    "kc_score = {}\n",
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(X_train_sc, y_train)\n",
    "    ca_score[name] = clf.score(X_val_sc, y_val)\n",
    "    ce_score[name] = log_loss(y_val, clf.predict_proba(X_val_sc))\n",
    "    kc_score[name] = cohen_kappa_score(y_val,clf.predict(X_val_sc))\n",
    "print('Classification performance on validation set:')\n",
    "for clf in names:\n",
    "    print (\"{}, accuracy: {:.3f}, log-loss: {:.3f}, kappa score: {:.3f}\".format(clf, ca_score[clf], ce_score[clf], kc_score[clf]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = [\"RBF SVC\", \"Logistic Regression\"]\n",
    "classifiers = [SVC(kernel='rbf', probability=True, random_state=random_state), LogisticRegression()]\n",
    "ca_score = {} # Classification accuracy\n",
    "ce_score = {} # Cross-entropy\n",
    "kc_score = {}\n",
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(X_train_sc, y_train)\n",
    "    ca_score[name] = clf.score(X_val_sc, y_val)\n",
    "    ce_score[name] = log_loss(y_val, clf.predict_proba(X_val_sc))\n",
    "    kc_score[name] = cohen_kappa_score(y_val,clf.predict(X_val_sc))\n",
    "print('Classification performance on validation set:')\n",
    "for clf in names:\n",
    "    print (\"{}, accuracy: {:.3f}, log-loss: {:.3f}, kappa score: {:.3f}\".format(clf, ca_score[clf], ce_score[clf], kc_score[clf]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
